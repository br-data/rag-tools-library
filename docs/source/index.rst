.. brdata-rag-tools documentation master file, created by
   sphinx-quickstart on Thu Dec  7 18:15:58 2023.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

.. highlight:: python

Welcome to brdata-rag-tools's documentation!
============================================

.. toctree::
   :maxdepth: 2
   :caption: Contents:

Tutorial
========

Hello to the brdata-rag-tools tutorial. In this brief introduction I will show you how to use the library with a brief example.

Installation
------------

Since the package is hosted on the test instance of pypi, you need to pass the index-url parameter to pip:

.. code-block:: bash

   python3 -m pip install --index-url https://test.pypi.org/simple/ --no-deps brdata-rag-tools

We install the package without dependencies since most of the dependencies are not hosted on the test instance of pypi. You may want to install it with the requirements.txt file from the github repository: https://github.com/br-data/rag-tools-library/blob/develop/docs/requirements.txt

.. code-block:: bash

   pip install -r https://raw.githubusercontent.com/br-data/rag-tools-library/develop/docs/requirements.txt

Basic Usage
-----------

First we choose which LLM we want to use by initiating the LLM class with a value from the LLMName enum.

All LLMs from brdata-rag-tools are connected via an API. The library serves as a wrapper to make the models more easily accessible.

.. code-block:: python

   from brdata_rag_tools.models import LLM, LLMName

   llm = LLM(model_name=LLMName.GPT35TURBO)

In this example we chose the GPT 3.5 Turbo model, but different flavours of GPT 3 and 4 are available, as well as the fine tuned on german text model IGEL and Google's Bison models.

All GPT models may be used by anyone with an API token, the IGEL and Bison Model is only accessible from BR Data's infrastructure.

Next we set the environment with OpenAI's access token:

.. code-block:: python
   os.environ["OPENAI_TOKEN"] = "YOUR TOKEN HERE"

For IGEL set the env var "IGEL_TOKEN" or "GOOGLE_TOKEN" for the Bison model respectively.

The LLM class holds merely the token and the actual model class.

The model class holds the actual logic and connections to interact with the language model endpoints.

Now we already can interact with the model via the prompt method:

.. code-block:: python

   joke = llm.model.prompt("Please tell me a joke.")
   print(joke)

Augmenting your prompt
----------------------

We do not only want to talk to our LLM, we want to augment it's prompt. This means we want to query a database for relevant content.

This is done using so called semantic, or vector, search. For semantic search the searchable content is transformed into a numerical representation, a vector embedding.

To retrieve relevant content, the user's prompt is also transformed into a vector. The prompt-vector is then compared to all vectors in the database and the most similar vectors are retrieved.

This vector based search needs specialized database types. Brdata-rag-tools currently supports only PGVector databases. The easiest way to run it, if you're not on the BR data infrastructure is to run it via docker:

.. code-block:: bash

   docker run -p 5432:5432 arkane/pgvector

Follow the instructions to set a password or trust all hosts.

If you're on the BR data infrastructure, simply add pgvector as database type to your project's config.yaml file and forward port 5432 to localhost.

Once you have your instance of pgvector running instance the PGVector class and supply it with the database's password.

.. code-block:: python

   from brdata_rag_tools.databases import PGVector
   database = PGVector(password="PASSWORD")

To search for relevant content, you first need to ingest it in the database.

Therefore you need a table in the database to ingest your data. You get a bare minimum of such a table with the following method:

.. code-block:: python

   embeding_table = database.create_abstract_embedding_table()

This method returns an abstract Database Table. Those table always contain the following Columns:

- id (string)
- embedding (Vector)
- embedding_source (string)

The embedding column will be generated by the Database from the content in embedding_source. The id needs to be unique for each row.

To actually use it, you need to inherit from the abstract table. In the following example, we will use our little search for podcast recommendations.

The table needs to know which kind of embedding you want to use. The most universal embedding type is Sentence Transformers, which is fine tuned for cosine similarity comparison of German texts.

.. code-block:: python

   from brdata_rag_tools.embeddings import EmbeddingType

   embedding = EmbeddingType.SENTENCE_TRANSFORMERS

   embeding_table = database.create_abstract_embedding_table(embed_type=embedding)

The returned abstract table is an SQLAlchemy table object. You may add your own Columns to it to store data additional to the three beforementioned items.

.. code-block:: python

   class Podcast(embedding_table):
       __tablename__ = "podcast"
       title: Mapped[str] = mapped_column(String)
       url: Mapped[str] = mapped_column(String)

Above we name the table as "podcast" in

Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`


Models
======

.. automodule:: models
   :members:

Databases
=========

Databases are vector databases for similarity search. Right now, only PGVector databases are supported.

.. automodule:: databases
   :members:

Embeddings
==========

.. automodule:: embeddings
   :members:
